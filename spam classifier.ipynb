{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score,f1_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{df.head()}\\n{df.shape}\\n{df.columns}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find number of null values in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning on the data permanently\n",
    "df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'v1':'label','v2':'email'},inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eda on the data\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert categorical column to numeric: using Label encoder\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicate values\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate values\n",
    "df.drop_duplicates(keep='first',inplace=True)\n",
    "df.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eda on the data\n",
    "df['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot ratio of ham vs spam\n",
    "sns.countplot(df['label'])\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot piechart with ratio of ham vs spam\n",
    "plt.pie(df['label'].value_counts(),labels=['ham','spam'],autopct='%0.1f%%')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the above corpus with word_tokenize and store it in a column named num_words\n",
    "df['num_words'] = df['email'].apply(lambda x: len(word_tokenize(x)))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_characters']=df['email'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_tokenize and store it in a column named num_sent\n",
    "df['num_sent'] = df['email'].apply(lambda x: len(sent_tokenize(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe ham\n",
    "df[df['label']==0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe spam\n",
    "df[df['label']==1].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram of num_characters for ham and spam using sns\n",
    "sns.distplot(df[df['label']==0]['num_characters'],label='ham')\n",
    "sns.distplot(df[df['label']==1]['num_characters'],label='spam')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairplot\n",
    "sns.pairplot(df,hue='label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation\n",
    "df.corr()\n",
    "#heatmap\n",
    "sns.heatmap(df.corr(),annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower case\n",
    "df['email'] = df['email'].apply(lambda x: x.lower())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "df['email'] = df['email'].apply(lambda x: word_tokenize(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove special characters\n",
    "\n",
    "df['email'] = df['email'].apply(lambda x: [re.sub(r'[^a-zA-Z0-9]','',i) for i in x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "df['email'] = df['email'].apply(lambda x: [i for i in x if i not in stop_words])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "\n",
    "df['email'] = df['email'].apply(lambda x: [i for i in x if i not in string.punctuation])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "\n",
    "ps = PorterStemmer()\n",
    "df['email'] = df['email'].apply(lambda x: [ps.stem(i) for i in x])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store transformed text into df['transformed_text']\n",
    "df['transformed_text'] = df['email']\n",
    "df['transformed_text'] \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversion of text into numeric vector using BOW\n",
    "\n",
    "cv = CountVectorizer(max_features=500)\n",
    "X = cv.fit_transform(df['transformed_text'].apply(lambda x: ' '.join(x)))\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into train and test\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model using Naive Bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred\n",
    "print(f'{accuracy_score(y_test,y_pred)},\\n{precision_score(y_test,y_pred)},\\n{confusion_matrix(y_test,y_pred)},\\n{recall_score(y_test,y_pred)},\\n{f1_score(y_test,y_pred)}')\n",
    "#get the heatmap of naive bayes\n",
    "sns.heatmap(confusion_matrix(y_test,y_pred), annot=True, cmap=\"Greens\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "plt.plot(recall, precision,'g')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model with gaussian naive bayes\n",
    "model = GaussianNB()\n",
    "model.fit(X_train.toarray(),y_train)\n",
    "y_pred = model.predict(X_test.toarray())\n",
    "y_pred\n",
    "print(f'{accuracy_score(y_test,y_pred)},\\n{precision_score(y_test,y_pred)},\\n{confusion_matrix(y_test,y_pred)},\\n{recall_score(y_test,y_pred)},\\n{f1_score(y_test,y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model with bernoulli naive bayes\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred\n",
    "print(f'{accuracy_score(y_test,y_pred)},\\n{precision_score(y_test,y_pred)},\\n{confusion_matrix(y_test,y_pred)}\\n{recall_score(y_test,y_pred)},\\n{f1_score(y_test,y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the accuracy score uding tfidf vectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=2500)\n",
    "X = tfidf.fit_transform(df['transformed_text'].apply(lambda x: ' '.join(x)))\n",
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversion of text into numeric vector using Tf-Idf\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=500)\n",
    "X = tfidf.fit_transform(df['transformed_text'].apply(lambda x: ' '.join(x)))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score,f1_score\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model using Naive Bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred\n",
    "print(accuracy_score(y_test,y_pred),\n",
    "precision_score(y_test,y_pred),\n",
    "confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model with gaussian naive bayes\n",
    "model = GaussianNB()\n",
    "model.fit(X_train.toarray(),y_train)\n",
    "y_pred = model.predict(X_test.toarray())\n",
    "y_pred\n",
    "print(accuracy_score(y_test,y_pred),\n",
    "precision_score(y_test,y_pred),\n",
    "confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model with bernoulli naive bayes\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred\n",
    "print(accuracy_score(y_test,y_pred),\n",
    "precision_score(y_test,y_pred),\n",
    "confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the accuracy score uding tfidf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=2500)\n",
    "X = tfidf.fit_transform(df['transformed_text'].apply(lambda x: ' '.join(x)))\n",
    "X.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
